@misc{shih2010arctic,
 title="Arctic Articulatory Database",
 author="Chilin Shih, Ryan Shosted, Torrey Loucks, Chris Carignan, and Panying Rong",
 abstract= "Articulograph, audio, and manually-checked time-aligned phone transcriptions for a selection of TIMIT and Arctic sentences. Sentences are selected to guarantee uniphone coverage, and to maximize diphone coverage of English. In order to encourage the speaker to produce interesting types of pronunciation variability, each sentence is recorded three times, with a listener at close, normal, and far from the speaker. Uniphone sentences are recorded nine times each = (3 distances) x (3 speaking styles), where the three speaking styles are accented, clear, and fast.",
 year="2010",
 img="https://speechtechnology.web.illinois.edu/media/online/data/arctic_articulatory-500x300.jpg",
 url="https://speechtechnology.web.illinois.edu/speech-production-research-initiative/",
 data="https://drive.google.com/drive/u/0/folders/1PHQXdyzhvWxzUc0WUcLTlQy4gV8_BXJa"
}

@misc{kuehn2001cadaver,
 title="Cadaver Tongue Specimen",
 data="https://drive.google.com/file/d/1YMdMIq86yiPGRpyQYaN-dHv6NA5wC2LB/view?usp=sharing",
 author="David Kuehn, Wei Tian, Victor Schepkin, Jerry Moon, and Mark Hasegawa-Johnson",
 img="https://speechtechnology.web.illinois.edu/media/online/data/tong3dhres2-500x300.jpg",
 abstract="Coronal MRI and Histologic sections of the same 1cm cubic specimen of excised cadaver tongue tissue. Coronal MRI sections, pixel size 59x59 microns, slice thickness 49 microns.",
 url="https://speechtechnology.web.illinois.edu/factor-analysis-of-the-tongue-shapes-of-speech/",
 year="2001"
}

@misc{ramnath2021fact,
 title="Fact-Based Spoken Visual Question Answering Database",
 author= "Kiran Ramnath, Leda SarÄ±, Mark Hasegawa-Johnson and Chang Yoo",
 doi="10.18653/v1/2021.naacl-main.153",
 year="2021",
 data="https://drive.google.com/file/d/1ls69C0oaElfkYJkOPFrbBTVD1xb-FeDC/view?usp=sharing",
 img="https://speechtechnology.web.illinois.edu/media/online/data/worldlywise_fig1-500x300.jpg",
 abstract="A dataset for Fact-based Spoken Visual Question Answering (FSVQA). Based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FSVQA, the question is spoken rather than typed. Spoken questions were generated by machine translation from English to Hindi or Turkish, followed by neural text-to-speech generation of speech ain English, Hindi, or Turkish."
}

@misc{hasegawajohnson2019ilocano,
 title="Illinois LORELEI Native Informant Speech: Ilocano",
 author="Mark Hasegawa-Johnson",
 url="https://speechtechnology.web.illinois.edu/languagenet-transfer-learning-across-a-language-similarity-network/",
 img="https://speechtechnology.web.illinois.edu/media/online/data/LangNet-500x300.jpg",
 abstract= "Several hundred sentences of newspaper text read by native speakers of Ilocano, Odia, and Sinhala",
 year="2019",
 data="https://drive.google.com/file/d/1G671IX2mr7P2iJJQOUtEbQqRRga69CUL/view",
}

@misc{hasegawajohnson2019odia,
 title="Illinois LORELEI Native Informant Speech: Odia",
 author="Mark Hasegawa-Johnson",
 url="https://speechtechnology.web.illinois.edu/languagenet-transfer-learning-across-a-language-similarity-network/",
 img="https://speechtechnology.web.illinois.edu/media/online/data/LangNet-500x300.jpg",
 abstract= "Several hundred sentences of newspaper text read by native speakers of Ilocano, Odia, and Sinhala",
 year="2019",
 data="https://drive.google.com/file/d/1AG0TJWu630t3f_JOjwAKo3qUOTvY_ysp/view",
}

@misc{hasegawajohnson2019sinhala,
 title="Illinois LORELEI Native Informant Speech: Sinhala",
 author="Mark Hasegawa-Johnson",
 url="https://speechtechnology.web.illinois.edu/languagenet-transfer-learning-across-a-language-similarity-network/",
 img="https://speechtechnology.web.illinois.edu/media/online/data/LangNet-500x300.jpg",
 abstract= "Several hundred sentences of newspaper text read by native speakers of Ilocano, Odia, and Sinhala",
 year="2019",
 data="https://drive.google.com/file/d/1egLGuj6we_QYs34bwCQGaul3aonLY6Xc/view"
}

@misc{hasegawajohnson2000infograms,
 title="Infograms",
 author= "Mark Hasegawa-Johnson",
 year= "2000",
 data="https://drive.google.com/file/d/1VgLEkmn3uJ4DkoG5FhW1cUqzs79mv0zr/view?usp=sharing",
 img="https://speechtechnology.web.illinois.edu/media/online/data/sample_infogram-500x300.jpg",
 doi="10.21437/ICSLP.2000-769",
 abstract="Infograms: Time-frequency distribution of the mutual information between an articulatory feature and the content of the spectrogram, averaged over the distribution of the TIMIT TRAIN corpus."
}

@misc{hasegawajohnson1999vowels,
 title="Vowels MRI Corpus",
 author= "Mark Hasegawa-Johnson, Shamala Pizza and Abeer Alwan",
 year= "1999",
 data="https://drive.google.com/file/d/1KLHcVW24ZMCfZ5XZL_CmO8wwcP1z237Q/view?usp=sharing",
 abstract="Coronal and axial image stacks portraying five different speakers during production of the ten monophthongal vowels of English. One speaker also produces a selection of consonants. Tongue and palate are outlined in the coronal image stack only. Voice recordings were made shortly before or after the imaging session, in both prone and sitting positions in a quiet recording studio.",
 url="https://reporter.nih.gov/search/A6RtagBSbEGeJ5AATGiLlw/project-details/2522259",
 img="https://speechtechnology.web.illinois.edu/media/online/data/adsagit-500x300.jpg"
}

@misc{yoon2009rated,
 title="Rated L2 Speech",
 author= "Su-Youn Yoon, Lisa Pierce, Amanda Huensch, Eric Juul, Samantha Perkins, Richard Sproat and Mark Hasegawa-Johnson",
 url="https://speechtechnology.web.illinois.edu/automatic-methods-for-second-language-fluency-assessment/",
 data="https://drive.google.com/file/d/1OsDGGKD68dPiqhlOjEDx2eY-YNCicO8W/view?usp=sharing",
 year= "2009",
 img="https://speechtechnology.web.illinois.edu/media/online/data/L2pron-500x300.jpg" 
}

@misc{lee2002room,
 title="Room Responses",
 author= "Bowon Lee, Camille Goudeseune and Mark Hasegawa-Johnson",
 img="https://speechtechnology.web.illinois.edu/media/online/data/capture1-500x300.jpg",
 data="https://drive.google.com/file/d/1lLKxSN7pu-ooqrjJCB60PCbw37zsgR7W/view?usp=sharing",
 abstract=" This directory lists impulse responses measured inside a plywood cube. The cube is six feet front to back, six feet top to bottom, and 5ft 10.5in left to right (+/- 2mm).  Microphone signal was recorded at four positions along a line drawn down the middle of the room. All microphone positions were 56.25 inches above the floor, midway between the left and right walls. Position 1 is 5in from the back wall, Position 2 is 16in, Position 3 is 26in, Position 4 is 36in.",
 year="2002",
 url="https://speechtechnology.web.illinois.edu/immersive-headphone-free-virtual-reality-audio/"
}

@misc{kim2013salient,
 title="Salient Events",
 author= "Kyungtae Kim, Kai-Hsiang Lin, Dirk B Walther, Mark A Hasegawa-Johnson and Thomas S Huang",
 doi="10.1016/j.patrec.2013.11.010",
 year="2013",
 img= "https://speechtechnology.web.illinois.edu/media/online/data/auditorysalience_distribution-500x300.jpg",
 data="https://drive.google.com/file/d/1gT0peFENQdeicD9qBh-hAfyqowR6d92h/view?usp=sharing",
 abstract="In this corpus ground truth for auditory salience is built up from annotations by human subjects of the AMI corpus of meeting room recordings. Twelve human annotators each labeled the times of perceptually salient events. A mixture-of-binomials model suggests that events detected by at least eight annotators should be considered perceptually salient."
}

@misc{hasegawajohnson2015ws15,
 title="WS15 Mismatched Crowdsourcing Dataset",
 author="Mark Hasegawa-Johnson, Preethi Jyothi, Daniel McCloy, Majid Mirbagheri, Giovanni di Liberto, Amit Das, Bradley Ekin, Chunxi Liu, Vimal Manohar, Hao Tang, Edmund C. Lalor, Nancy F. Chen, Paul Hager, Tyler Kekona, Rose Sloan, Adrian KC Lee, Boon Pang Lim and Wenda Chen",
 doi="10.1109/TASLP.2016.2621659",
 year="2015",
 abstract="Transcriptions of non-English audio by monolingual English-speaking crowd workers.",
 data="https://drive.google.com/drive/u/0/folders/1PHQXdyzhvWxzUc0WUcLTlQy4gV8_BXJa",
 img="https://speechtechnology.web.illinois.edu/media/online/data/mismatched_crowdsourcing-500x300.jpg",
 url="https://www.clsp.jhu.edu/workshops/15-workshop/probabilistic-transcription-of-languages-with-no-native-language-transcribers/"
}

@misc{lim2010wtimit,
 title="wTIMIT Whispered TIMIT dataset",
 author="Boon Pang Lim",
 year="2010",
 abstract="Whispered and normally voiced sentences read by native speakers of Singapore English and US English",
 data="https://drive.google.com/file/d/1k781xbiTmRHz8ilMlk1H0cAg5bFWRFDd/view?usp=sharing",
 img="https://speechtechnology.web.illinois.edu/media/online/data/wtimit-500x300.jpg",
 url="http://www.isle.illinois.edu/speech_web_lg/pubs/2010/lim10thesis.pdf"
}

@misc{lee2004avicar,
 title="AVICAR: Audiovisual Speech Recognition in a Car",
 img="https://speechtechnology.web.illinois.edu/media/online/data/inside-500x300.jpg",
 doi="10.21437/Interspeech.2004-424",
 author="Bowon Lee, Mark Hasegawa-Johnson, Camille Goudeseune, Suketu Kamdar, Sarah Borys, Ming Liu, and Thomas Huang",
 abstract="Speech recorded using 4 Cameras in a lateral array on the dashboard, 7 Microphones in a lateral array on the sunvisor, 5 Noise Conditions (engine idling=IDL, 35mph with the windows up=35U, 35mph with the windows down=35D, 55mph with the windows up=55U, 55mph with the windows down=55D), 4 Types of Read Speech (isolated digits=D, isolated letters=L, ten-digit phone numbers=P, TIMIT sentences=S), 86 Talkers including 46 men and 40 women. The AVICAR corpus is available to researchers interested in speech recognition research. Please contact Professor Hasegawa-Johnson to request download access.",
 year="2004",
 url="https://speechtechnology.web.illinois.edu/audiovisual-speech-recognition-data-collection-and-feature-extraction-in-automotive-environment/"

}

@misc{kim2008uaspeech,
 title="UASpeech: Universal Access Speech Corpus",
 abstract="Audio and video recordings of 17 participants with Cerebral Palsy and 17 age and gender-matched controls reading isolated digits, isolated letters of the international radio alphabet, computer command words, 100 common words, and 300 phonetically balanced uncommon words. The UASpeech corpus is available to researchers from academic or government laboratories. Please contact Professor Hasegawa-Johnson to request download access.",
 url="https://speechtechnology.web.illinois.edu/audiovisual-description-and-recognition-of-audible-and-visible-dysarthric-phonology/",
 img="https://speechtechnology.web.illinois.edu/media/online/data/beckmanOH09_1-500x300.jpg",
 doi="10.21437/Interspeech.2008-480",
 author="Heejin Kim, Mark Hasegawa-Johnson, Adrienne Perlman, Jon Gunderson, Thomas S. Huang, Kenneth Watkin, and Simone Frame",
 year="2008"
}
